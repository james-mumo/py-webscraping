{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1575953c-285a-4a42-ad33-1a756b28bf2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42419\n"
     ]
    }
   ],
   "source": [
    "# we grab the html data using request library\n",
    "import requests\n",
    "r = requests.get('https://www.usclimatedata.com/climate/united-states/us')\n",
    "# lets check the length\n",
    "print(len(r.text))\n",
    "# r.text <_ that is the html page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851bb9ed-5b47-4ae3-9551-d5711a366b08",
   "metadata": {},
   "source": [
    "### use bs4 to convert the html to bs4 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff8b75f-ca4d-4e26-9ca4-8afd803a5649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Climate United States - Normals and averages</title>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "# lets check the title of the html page\n",
    "print(soup.title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e202671-0014-4b97-90eb-57baa8d5a9af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate United States - Normals and averages\n"
     ]
    }
   ],
   "source": [
    "# lets just get the actual string\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a1c5a-05f2-4cab-82ab-53e755bf6716",
   "metadata": {},
   "source": [
    "### we can pretify the whole html page from the bs4 object and print it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f6b6b7-6ad0-46a1-a8e2-51bb729ced15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df0a54-29d3-4a2c-b0cb-d7d9d97c0b9e",
   "metadata": {},
   "source": [
    "### In this page there are links to other states and therefore\n",
    "### in these links are 50 states with climate data hence leths get the links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f102a496-f2cb-4ec3-bb6b-897ade58ee95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "#\n",
      "/\n",
      "/climate/united-states/us\n",
      "/\n",
      "/climate/united-states/us\n",
      "/climate/alabama/united-states/3170\n",
      "/climate/alaska/united-states/3171\n",
      "/climate/arizona/united-states/3172\n",
      "/climate/arkansas/united-states/3173\n",
      "/climate/california/united-states/3174\n",
      "/climate/colorado/united-states/3175\n",
      "/climate/connecticut/united-states/3176\n",
      "/climate/delaware/united-states/3177\n",
      "/climate/district-of-columbia/united-states/3178\n",
      "/climate/florida/united-states/3179\n",
      "/climate/georgia/united-states/3180\n",
      "/climate/hawaii/united-states/3181\n",
      "/climate/idaho/united-states/3182\n",
      "/climate/illinois/united-states/3183\n",
      "/climate/indiana/united-states/3184\n",
      "/climate/iowa/united-states/3185\n",
      "/climate/kansas/united-states/3186\n",
      "/climate/kentucky/united-states/3187\n",
      "/climate/louisiana/united-states/3188\n",
      "/climate/maine/united-states/3189\n",
      "/climate/maryland/united-states/1872\n",
      "/climate/massachusetts/united-states/3191\n",
      "/climate/michigan/united-states/3192\n",
      "/climate/minnesota/united-states/3193\n",
      "/climate/mississippi/united-states/3194\n",
      "/climate/missouri/united-states/3195\n",
      "/climate/montana/united-states/919\n",
      "/climate/nebraska/united-states/3197\n",
      "/climate/nevada/united-states/3198\n",
      "/climate/new-hampshire/united-states/3199\n",
      "/climate/new-jersey/united-states/3200\n",
      "/climate/new-mexico/united-states/3201\n",
      "/climate/new-york/united-states/3202\n",
      "/climate/north-carolina/united-states/3203\n",
      "/climate/north-dakota/united-states/3204\n",
      "/climate/ohio/united-states/3205\n",
      "/climate/oklahoma/united-states/3206\n",
      "/climate/oregon/united-states/3207\n",
      "/climate/pennsylvania/united-states/3208\n",
      "/climate/puerto-rico/united-states/7335\n",
      "/climate/rhode-island/united-states/3209\n",
      "/climate/south-carolina/united-states/3210\n",
      "/climate/south-dakota/united-states/3211\n",
      "/climate/tennessee/united-states/3212\n",
      "/climate/texas/united-states/3213\n",
      "/climate/utah/united-states/3214\n",
      "/climate/vermont/united-states/3215\n",
      "/climate/virginia/united-states/3216\n",
      "/climate/washington/united-states/3217\n",
      "/climate/west-virginia/united-states/3218\n",
      "/climate/wisconsin/united-states/3219\n",
      "/climate/wyoming/united-states/3220\n",
      "/climate/washington/district-of-columbia/united-states/usdc0001\n",
      " https://facebook.com/sharer/sharer.php?u=https://www.usclimatedata.com/climate/united-states/us\n",
      "https://twitter.com/intent/tweet/?text=Climate+United+States+-+Normals+and+averages&url=https://www.usclimatedata.com/climate/united-states/us\n",
      "https://pinterest.com/pin/create/button/?url=https://www.usclimatedata.com/climate/united-states/us&media=description=Climate+United+States+-+Normals+and+averages\n",
      "mailto:mail@example.com?subject=Climate+United+States+-+Normals+and+averages&body=Climate+United+States+-+Normals+and+averages+-+https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus\n",
      "https://www.tumblr.com/widgets/share/tool?posttype=link&title=Climate+United+States+-+Normals+and+averages&caption=&content=Climate+United+States+-+Normals+and+averages+-+https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus&canonicalUrl=https://www.usclimatedata.com/climate/united-states/us&shareSource=tumblr_share_button\n",
      "whatsapp://send?text=Climate+United+States+-+Normals+and+averages+-+https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus\n",
      "https://www.facebook.com/yourweatherservice\n",
      "https://twitter.com/usclimatedata\n",
      "/website-info\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69326a38-84ee-4cfd-85ff-3f9b15400a03",
   "metadata": {},
   "source": [
    "### Filter the urls to get the ony ones we need\n",
    "we can use an if statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf7b3c9-1107-4ad2-9387-4e6cedbc3832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# empty py list that we append later\n",
    "state_links = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    url = link.get('href')\n",
    "    if url and '/climate' in url and '/climate/united-states/us' not in url:\n",
    "        state_links.append(url)\n",
    "        \n",
    "# lets check if we have all states url\n",
    "print(len(state_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd306075-b9eb-468f-ac8f-061e84b430de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alabama\n",
      "alaska\n",
      "arizona\n",
      "arkansas\n",
      "california\n",
      "colorado\n",
      "connecticut\n",
      "delaware\n",
      "district-of-columbia\n",
      "florida\n",
      "georgia\n",
      "hawaii\n",
      "idaho\n",
      "illinois\n",
      "indiana\n",
      "iowa\n",
      "kansas\n",
      "kentucky\n",
      "louisiana\n",
      "maine\n",
      "maryland\n",
      "massachusetts\n",
      "michigan\n",
      "minnesota\n",
      "mississippi\n",
      "missouri\n",
      "montana\n",
      "nebraska\n",
      "nevada\n",
      "new-hampshire\n",
      "new-jersey\n",
      "new-mexico\n",
      "new-york\n",
      "north-carolina\n",
      "north-dakota\n",
      "ohio\n",
      "oklahoma\n",
      "oregon\n",
      "pennsylvania\n",
      "puerto-rico\n",
      "rhode-island\n",
      "south-carolina\n",
      "south-dakota\n",
      "tennessee\n",
      "texas\n",
      "utah\n",
      "vermont\n",
      "virginia\n",
      "washington\n",
      "west-virginia\n",
      "wisconsin\n",
      "wyoming\n",
      "washington\n"
     ]
    }
   ],
   "source": [
    "# maybe list them all\n",
    "for state in state_links:\n",
    "    stateName = state.split(\"/\")[2]\n",
    "    print(stateName)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa23d56-c537-40a2-adb1-6dc99f2bd1c1",
   "metadata": {},
   "source": [
    "### Testing dat for one state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03ad65b-a251-4fa5-8b88-42868348269d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Alabama - Temperature, Rainfall and Averages\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.usclimatedata.com'\n",
    "r = requests.get(base_url + state_links[0])\n",
    "soup = BeautifulSoup(r.text)\n",
    "print(soup.title.string)\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e70883-8407-4cc4-9fa7-1f8b558644a6",
   "metadata": {},
   "source": [
    "### the data is the table tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "093a7b06-5622-4865-8746-6455db533f17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "rows = soup.find_all('tr')\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed025296-2ed8-4990-ad79-2f6090bba827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['54', '58', '67', '74', '82', '88', '91', '91', '85', '75', '65', '56']\n"
     ]
    }
   ],
   "source": [
    "rows = [row for row in rows if 'Average high' in str(row)]\n",
    "print(len(rows))\n",
    "\n",
    "high_temps = []\n",
    "for row in rows:\n",
    "    tds = row.find_all('td')\n",
    "    for i in range(1,7):\n",
    "        high_temps.append(tds[i-1].text)\n",
    "print(high_temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2ba1b-1f77-4b31-8f08-7a4c66e29748",
   "metadata": {},
   "source": [
    "### Get the name of the State\n",
    "First attempt we just split the title string into a list, and grab the second word.\n",
    "But that doesn't work for 2-word states like New York and North Carolina.\n",
    "So instead we slice the string from first blank to the hyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec5eff8-64ca-4073-8443-dd679ee53327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama\n",
      "Alabama\n"
     ]
    }
   ],
   "source": [
    "state = soup.title.string.split()[1]\n",
    "print(state)\n",
    "s = soup.title.string\n",
    "state = s[s.find(' '):s.find('-')].strip()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb7080-dea8-4c12-88f3-4cf1532fc1eb",
   "metadata": {},
   "source": [
    "### Add state name and temp list to the data dictionary\n",
    "For a single state, this is what our scraped data looks like.\n",
    "In this example we only got monthly highs by state, but you could drill into cities, and could get lows and precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc2d346-74b0-4607-b9f8-8a898602bbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alabama': ['54', '58', '67', '74', '82', '88', '91', '91', '85', '75', '65', '56']}\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "data[state] = high_temps\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ef42f-fc51-4725-8885-097dff9a1bbe",
   "metadata": {},
   "source": [
    "## Put it all together and iterate 51 states\n",
    "We loop through our 51-state list, and get high temp data for each state, and add it to the data dict.\n",
    "This combines all our work above into a single for loop.\n",
    "The result is a dict with 51 states and a list of monthly highs for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca66483d-12a9-499d-81fc-06728be3f74c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alabama': ['54', '58', '67', '74', '82', '88', '91', '91', '85', '75', '65', '56'], 'Alaska': ['23', '27', '34', '44', '56', '63', '65', '64', '55', '40', '28', '25'], 'Arizona': ['67', '71', '77', '85', '95', '104', '106', '104', '100', '89', '76', '66'], 'Arkansas': ['51', '55', '64', '73', '81', '89', '92', '93', '86', '75', '63', '52'], 'California': ['54', '60', '65', '71', '80', '87', '92', '91', '87', '78', '64', '54'], 'Colorado': ['45', '46', '54', '61', '72', '82', '90', '88', '79', '66', '52', '45'], 'Connecticut': ['37', '40', '47', '58', '68', '77', '82', '81', '74', '63', '53', '42'], 'Delaware': ['43', '47', '55', '66', '75', '83', '87', '85', '79', '69', '58', '47'], 'District Of Columbia': ['42', '44', '53', '64', '75', '83', '87', '84', '78', '67', '55', '45'], 'Florida': ['64', '67', '74', '80', '87', '91', '92', '92', '88', '81', '73', '65'], 'Georgia': ['52', '57', '64', '72', '81', '86', '90', '88', '82', '73', '64', '54'], 'Hawaii': ['80', '80', '81', '83', '85', '87', '88', '89', '89', '87', '84', '81'], 'Idaho': ['38', '45', '55', '62', '72', '81', '91', '90', '79', '65', '48', '38'], 'Illinois': ['32', '36', '46', '59', '70', '81', '84', '82', '75', '63', '48', '36'], 'Indiana': ['35', '40', '51', '63', '73', '82', '85', '83', '77', '65', '52', '39'], 'Iowa': ['31', '36', '49', '62', '72', '82', '86', '84', '76', '63', '48', '34'], 'Kansas': ['40', '45', '56', '67', '76', '85', '89', '89', '80', '68', '55', '42'], 'Kentucky': ['40', '45', '55', '66', '75', '83', '87', '86', '79', '68', '55', '44'], 'Louisiana': ['62', '65', '72', '78', '85', '89', '91', '91', '87', '80', '72', '64'], 'Maine': ['28', '32', '40', '53', '65', '74', '79', '78', '70', '57', '45', '33'], 'Maryland': ['42', '46', '54', '65', '75', '85', '89', '87', '80', '68', '58', '46'], 'Massachusetts': ['36', '39', '45', '56', '66', '76', '81', '80', '72', '61', '51', '41'], 'Michigan': ['30', '33', '44', '58', '69', '78', '82', '80', '73', '60', '47', '34'], 'Minnesota': ['26', '31', '43', '58', '71', '80', '85', '82', '73', '59', '42', '29'], 'Mississippi': ['56', '60', '69', '76', '83', '89', '92', '92', '87', '77', '67', '58'], 'Missouri': ['40', '45', '56', '67', '75', '83', '88', '88', '80', '69', '56', '43'], 'Montana': ['33', '39', '48', '58', '67', '76', '86', '85', '73', '59', '43', '32'], 'Nebraska': ['32', '37', '50', '63', '73', '84', '88', '86', '77', '64', '48', '36'], 'Nevada': ['45', '50', '57', '63', '71', '81', '90', '88', '80', '68', '54', '45'], 'New Hampshire': ['31', '35', '44', '57', '69', '77', '82', '81', '73', '60', '48', '36'], 'New Jersey': ['39', '42', '51', '62', '72', '82', '86', '84', '77', '65', '55', '44'], 'New Mexico': ['44', '48', '56', '65', '74', '83', '86', '83', '78', '67', '53', '43'], 'New York': ['39', '42', '50', '60', '71', '79', '85', '83', '76', '65', '54', '44'], 'North Carolina': ['51', '55', '63', '72', '79', '86', '89', '87', '81', '72', '62', '53'], 'North Dakota': ['23', '28', '40', '57', '68', '77', '85', '83', '72', '58', '40', '26'], 'Ohio': ['36', '40', '52', '63', '73', '82', '85', '84', '77', '65', '52', '41'], 'Oklahoma': ['50', '55', '63', '72', '80', '88', '94', '93', '85', '73', '62', '51'], 'Oregon': ['48', '52', '56', '61', '68', '74', '82', '82', '77', '64', '53', '46'], 'Pennsylvania': ['40', '44', '53', '64', '74', '83', '87', '85', '78', '67', '56', '45'], 'Puerto Rico': ['82', '83', '83', '85', '86', '88', '87', '88', '88', '87', '85', '83'], 'Rhode Island': ['37', '40', '48', '59', '68', '78', '83', '81', '74', '63', '53', '42'], 'South Carolina': ['59', '63', '70', '76', '83', '88', '91', '89', '85', '77', '70', '62'], 'South Dakota': ['22', '27', '39', '57', '69', '78', '84', '82', '72', '58', '39', '25'], 'Tennessee': ['50', '55', '64', '73', '81', '89', '92', '91', '85', '74', '63', '52'], 'Texas': ['62', '65', '72', '80', '87', '92', '96', '97', '91', '82', '71', '63'], 'Utah': ['38', '44', '53', '61', '71', '82', '90', '89', '78', '65', '50', '40'], 'Vermont': ['27', '31', '40', '55', '67', '76', '81', '79', '70', '57', '46', '33'], 'Virginia': ['47', '51', '60', '70', '78', '86', '90', '88', '81', '71', '61', '51'], 'Washington': ['42', '44', '53', '64', '75', '83', '87', '84', '78', '67', '55', '45'], 'West Virginia': ['42', '47', '56', '68', '75', '82', '85', '84', '78', '68', '57', '46'], 'Wisconsin': ['29', '33', '42', '54', '65', '75', '80', '78', '71', '59', '46', '33'], 'Wyoming': ['40', '40', '47', '55', '65', '75', '83', '81', '72', '59', '47', '38']}\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for state_link in state_links:\n",
    "    url = base_url + state_link\n",
    "    r = requests.get(base_url + state_link)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    rows = soup.find_all('tr')\n",
    "    rows = [row for row in rows if 'Average high' in str(row)]\n",
    "    high_temps = []\n",
    "    for row in rows:\n",
    "        tds = row.find_all('td')\n",
    "        for i in range(1,7):\n",
    "            high_temps.append(tds[i-1].text)\n",
    "    s = soup.title.string\n",
    "    state = s[s.find(' '):s.find('-')].strip()\n",
    "    data[state] = high_temps\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b42380-9b4f-4072-8ea5-cb4a967dcb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f8ae96d-3f2e-4bf9-87d7-0216a2c11f88",
   "metadata": {},
   "source": [
    "## Saving the Data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd2f8860-0ed4-4cc6-a1ed-df4f80558c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('high_temps.csv','w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36f3ea-d551-4c99-aebf-0fd0e1fa2333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
